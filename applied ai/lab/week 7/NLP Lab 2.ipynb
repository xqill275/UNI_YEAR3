{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH3GMNUyHHP7"
   },
   "outputs": [],
   "source": [
    "# This notebook demonstrates fundamental of This chapter covered key preprocessing techniques, including\n",
    "# stop word removal, stemming, lemmatization, regular expressions, and tokenization.\n",
    "# Each exercise focuses on a specific preprocessing technique:\n",
    "# 1.  **Stop Word Removal**: Eliminating common words (like 'the', 'is', 'a') that typically do not carry significant meaning.\n",
    "# 2.  **Stemming**: Reducing words to their root or base form (stem) by chopping off suffixes.\n",
    "# 3.  **Lemmatization**: Reducing words to their dictionary or lemma form, which is a linguistically valid base form.\n",
    "# 4.  **Tokenization**: Breaking down text into smaller units called tokens (words or sentences).\n",
    "# 5.  **Part-of-Speech (POS) Tagging**: Assigning grammatical categories (e.g., noun, verb, adjective) to each word.\n",
    "# 6.  **Frequency Distribution**: Counting the occurrences of each unique word or token in a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8490fe4a"
   },
   "source": [
    "## NLP Preprocessing Techniques\n",
    "\n",
    "Preprocessing is a crucial step in Natural Language Processing (NLP) workflows. It involves cleaning and transforming raw text data into a format that is more suitable for machine learning models and analytical tasks. Each technique plays a vital role in enhancing the quality and efficiency of NLP applications:\n",
    "\n",
    "*   **Stop Word Removal:**\n",
    "    *   **Importance:** Reduces noise in text data by eliminating common words that typically don't carry significant meaning (e.g., 'the', 'is', 'a'). This helps in focusing on more important terms for analysis, reduces the dimensionality of the data, and improves the performance of algorithms by removing irrelevant features.\n",
    "\n",
    "*   **Stemming:**\n",
    "    *   **Importance:** Groups words with similar meanings (e.g., 'connect', 'connecting', 'connected' all reduce to 'connect'). This helps in reducing the vocabulary size, standardizing words, and can improve recall in information retrieval systems by matching different forms of a word to a single root.\n",
    "\n",
    "*   **Lemmatization:**\n",
    "    *   **Importance:** Similar to stemming, it reduces words to their base dictionary form (lemma), but ensures the resulting word is a linguistically valid term. This provides a more accurate representation of words, which is crucial for applications requiring high linguistic precision, such as machine translation or question-answering systems.\n",
    "\n",
    "*   **Tokenization:**\n",
    "    *   **Importance:** Breaks down raw text into fundamental units (tokens like words or sentences). This is the very first step in almost any NLP task, as it converts unstructured text into structured elements that can be further processed and analyzed by algorithms.\n",
    "\n",
    "*   **Part-of-Speech (POS) Tagging:**\n",
    "    *   **Importance:** Assigns grammatical categories to each word (e.g., noun, verb, adjective). This contextual information is invaluable for understanding the syntactic structure of a sentence, disambiguating word meanings (e.g., 'bank' as a financial institution vs. river bank), and is often a prerequisite for more advanced NLP tasks like named entity recognition and parsing.\n",
    "\n",
    "*   **Frequency Distribution:**\n",
    "    *   **Importance:** Provides insights into the most common words or tokens in a corpus. This helps in identifying key themes, prevalent topics, and important terms within the text, which is useful for feature selection, keyword extraction, and understanding the overall composition of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75d9a21a"
   },
   "source": [
    "### Exercise 1: Stop Word Removal\n",
    "\n",
    "**Task:** Use the `nltk` library to remove stop words from the following text: \"NLP enables computers to understand human language, which is a crucial aspect of artificial intelligence.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "11ffbb1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['NLP', 'enables', 'computers', 'to', 'understand', 'human', 'language,', 'which', 'is', 'a', 'crucial', 'aspect', 'of', 'artificial', 'intelligence.']\n",
      "\n",
      "Filtered Tokens:\n",
      "['NLP', 'enables', 'computers', 'understand', 'human', 'language,', 'crucial', 'aspect', 'artificial', 'intelligence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"NLP enables computers to understand human language, which is a crucial aspect of artificial intelligence.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nFiltered Tokens:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7df16cc6"
   },
   "source": [
    "### Challenge 1: Stop Word Removal with Punctuation\n",
    "\n",
    "**Task:** Modify the stop word removal exercise to also remove punctuation from the text before filtering stop words.\n",
    "\n",
    "**Hint:** You might need the `string` module and `str.translate` or `re` module for punctuation removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb4dbga0njR0"
   },
   "source": [
    "###Exercise 2: Stemming\n",
    "**Task:** Use the nltk library to perform stemming on the following text: \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d2f85bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['Stemming', 'helps', 'in', 'reducing', 'words', 'to', 'their', 'root', 'form,', 'which', 'can', 'be', 'beneficial', 'for', 'text', 'processing.']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['stem', 'help', 'in', 'reduc', 'word', 'to', 'their', 'root', 'form', 'which', 'can', 'be', 'benefici', 'for', 'text', 'processing']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Sample text\n",
    "text = \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# remove punctuation from stemmed tokens\n",
    "stemmed_tokens = [word.translate(str.maketrans('', '', string.punctuation)) for word in stemmed_tokens]\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nStemmed Tokens:\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1112db8a"
   },
   "source": [
    "### Challenge 2: Compare Stemmers\n",
    "\n",
    "**Task:** Implement stemming using another stemmer from `nltk.stem` (e.g., `SnowballStemmer` or `LancasterStemmer`) on the same text. Compare the results with the `PorterStemmer` output and note any differences.\n",
    "\n",
    "**Hint:** Remember to import the new stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snowball Stemmed Tokens:\n",
      "['stem', 'help', 'in', 'reduc', 'word', 'to', 'their', 'root', 'form,', 'which', 'can', 'be', 'benefici', 'for', 'text', 'processing.']\n"
     ]
    }
   ],
   "source": [
    "# using snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "text = \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n",
    "tokens = text.split()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stemmed_tokens = [snowball_stemmer.stem(word) for word in tokens]\n",
    "print(\"\\nSnowball Stemmed Tokens:\")\n",
    "print(snowball_stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa6e5491"
   },
   "source": [
    "### Exercise 3: Lemmatization\n",
    "\n",
    "**Task:** Use the `nltk` library to perform lemmatization on the following text: \"Lemmatization aims to reduce words to their base or dictionary form, which is useful for linguistic analysis.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4de27c0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['Lemmatization', 'aims', 'to', 'reduce', 'words', 'to', 'their', 'base', 'or', 'dictionary', 'form', ',', 'which', 'is', 'useful', 'for', 'linguistic', 'analysis', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['Lemmatization', 'aim', 'to', 'reduce', 'word', 'to', 'their', 'base', 'or', 'dictionary', 'form', ',', 'which', 'be', 'useful', 'for', 'linguistic', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary data for lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Added to resolve LookupError for averaged_perceptron_tagger_eng\n",
    "\n",
    "# Sample text\n",
    "text = \"Lemmatization aims to reduce words to their base or dictionary form, which is useful for linguistic analysis.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun if no clear tag\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatize the tokens with POS tags\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60b0251"
   },
   "source": [
    "### Challenge 3: Lemmatize Different POS Tags\n",
    "\n",
    "**Task:** Extend the lemmatization exercise by choosing one word that can be a noun and a verb (e.g., 'runs', 'running') and demonstrate how lemmatization changes based on the assigned POS tag.\n",
    "\n",
    "**Hint:** You'll need to explicitly pass the correct WordNet POS tag (`wordnet.NOUN` or `wordnet.VERB`) to the `lemmatize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: 'running'\n",
      "Lemma as Noun: 'running'\n",
      "Lemma as Verb: 'run'\n"
     ]
    }
   ],
   "source": [
    "''' ### Challenge 3: Lemmatize Different POS Tags\n",
    "\n",
    "**Task:** Extend the lemmatization exercise by choosing one word that can be a noun and a verb (e.g., 'runs', 'running') and demonstrate how lemmatization changes based on the assigned POS tag.\n",
    "\n",
    "**Hint:** You'll need to explicitly pass the correct WordNet POS tag (`wordnet.NOUN` or `wordnet.VERB`) to the `lemmatize` function.'''\n",
    "\n",
    "word = \"running\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_as_noun = lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "lemma_as_verb = lemmatizer.lemmatize(word, wordnet.VERB)    \n",
    "print(f\"\\nWord: '{word}'\")\n",
    "print(f\"Lemma as Noun: '{lemma_as_noun}'\")\n",
    "print(f\"Lemma as Verb: '{lemma_as_verb}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri2qoFAsoQV3"
   },
   "source": [
    "###Exercise 4: Tokenization\n",
    "**Task:** Use the nltk library to perform word and sentence tokenization on the following paragraph: \"Tokenization is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens. The goal is to make it easier for computers to process natural language.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d2f7edf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Tokenization is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens. The goal is to make it easier for computers to process natural language.\n",
      "\n",
      "Word Tokens:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'a', 'stream', 'of', 'text', 'into', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'meaningful', 'elements', 'called', 'tokens', '.', 'The', 'goal', 'is', 'to', 'make', 'it', 'easier', 'for', 'computers', 'to', 'process', 'natural', 'language', '.']\n",
      "\n",
      "Sentence Tokens:\n",
      "['Tokenization is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens.', 'The goal is to make it easier for computers to process natural language.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary data for tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Added to resolve LookupError for punkt_tab\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens. The goal is to make it easier for computers to process natural language.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nWord Tokens:\")\n",
    "print(word_tokens)\n",
    "print(\"\\nSentence Tokens:\")\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6e56d4f"
   },
   "source": [
    "### Challenge 4: Regex Tokenization\n",
    "\n",
    "**Task:** Instead of `nltk.word_tokenize`, use `nltk.regexp_tokenize` to tokenize the sample text. Create a regular expression that captures words (alphanumeric sequences) and punctuation marks as separate tokens.\n",
    "\n",
    "**Hint:** A pattern like `r'\\w+|\\S'` might be a good starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI2EEGkuoj85"
   },
   "source": [
    "###Exercise 5: Part-of-Speech (POS) Tagging\n",
    "**Task:** Use the nltk library to perform Part-of-Speech (POS) tagging on the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_5088\\2663584822.py:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  sentence = \"r'\\w+|\\S\"\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "regexp_tokenize() missing 1 required positional argument: 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m sentence = \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Tokenize the sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregexp_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Perform POS tagging\u001b[39;00m\n\u001b[32m     15\u001b[39m pos_tags = nltk.pos_tag(tokens)\n",
      "\u001b[31mTypeError\u001b[39m: regexp_tokenize() missing 1 required positional argument: 'pattern'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary data for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Added to resolve LookupError for averaged_perceptron_tagger_eng\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"r'\\w+|\\S\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.regexp_tokenize(sentence, r'\\w+')\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nPOS Tags:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a806c134"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary data for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Added to resolve LookupError for averaged_perceptron_tagger_eng\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nPOS Tags:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffb9aac9"
   },
   "source": [
    "### Challenge 5: Detailed POS Tagging Analysis\n",
    "\n",
    "**Task:** For the given sentence, not only display the POS tags but also explain what each tag means (e.g., 'DT' for determiner, 'JJ' for adjective, 'NN' for noun, 'VBZ' for verb, 3rd person singular present).\n",
    "\n",
    "**Hint:** You can find a list of NLTK POS tags and their meanings in the NLTK documentation or by searching online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZlKOxTMo8gz"
   },
   "source": [
    "###Exercise 6: Frequency Distribution\n",
    "**Task:** Calculate the frequency distribution of words in the following text, and display the 5 most common words: \"Natural language processing (NLP) is a field of artificial intelligence, machine learning, and deep learning. NLP helps computers understand and process human language.\"\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "993d2df1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab') # Added to resolve LookupError for punkt_tab\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing (NLP) is a field of artificial intelligence, machine learning, and deep learning. NLP helps computers understand and process human language.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text.lower()) # Convert to lowercase for consistent counting\n",
    "\n",
    "# Remove stop words and punctuation (optional but good for freq dist)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = FreqDist(filtered_tokens)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nFiltered Tokens for Frequency Distribution:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nMost Common 5 Words:\")\n",
    "print(fdist.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0b63886"
   },
   "source": [
    "### Challenge 6: Frequency Distribution with Bigrams\n",
    "\n",
    "**Task:** Instead of individual word frequency, calculate the frequency distribution of bigrams (sequences of two words) in the sample text. Display the 5 most common bigrams.\n",
    "\n",
    "**Hint:** You can use `nltk.bigrams` to generate bigrams from your filtered tokens."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJ+KbUH2maFtv7gdSTADE6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
