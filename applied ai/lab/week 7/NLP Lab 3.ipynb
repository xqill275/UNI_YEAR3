{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOfkH2jpRENxqrWAEztPjp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This notebook contains a series of exercises designed to help you\n","# understand and implement fundamental techniques in Natural Language Processing (NLP) feature engineering using scikit-learn.\n","\n","# Exercises:\n","# 1.  **Bag of Words:** Learn how to transform text data into a Bag of Words representation.\n","# 2.  **TF-IDF:** Explore the Term Frequency-Inverse Document Frequency (TF-IDF) method for text representation."],"metadata":{"id":"YI6t3KhGYWVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0a02be73"},"source":["### Exercise 1: Bag of Words\n","\n","**Task:** Use the `CountVectorizer` from scikit-learn to transform the following text corpus into a Bag of Words representation:\n","\n","```python\n","documents = [\n","    \"Text processing is important for NLP.\",\n","    \"Bag of Words is a simple text representation method.\",\n","    \"Feature engineering is essential in machine learning.\"\n","]\n","```\n"]},{"cell_type":"markdown","source":["The Bag of Words model represents text as an unordered collection of words, disregarding grammar and even word order, but keeping track of word frequencies. Essentially, it creates a vocabulary of all unique words in the corpus and then for each document, it counts how many times each word from the vocabulary appears."],"metadata":{"id":"2vwWWEqFsQ5L"}},{"cell_type":"code","metadata":{"id":"1e15c702"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Text processing is important for NLP.\",\n","    \"Bag of Words is a simple text representation method.\",\n","    \"Feature engineering is essential in machine learning.\"\n","]\n","\n","# Initialize the CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","bow_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary:\")\n","print(vocab)\n","print(\"\\nBag of Words Array:\")\n","print(bow_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9387d77"},"source":["### Challenge 1: Bag of Words\n","\n","**Task:** Using the `vectorizer` from Exercise 1, transform the following new document into its Bag of Words representation and print the result:\n","\n","```python\n","new_document_bow = [\"NLP is an important field.\"]\n","```\n","\n","**Hint:** You'll need to use the `transform` method on the `vectorizer` that was already fitted on the original corpus."]},{"cell_type":"markdown","metadata":{"id":"8916b9b3"},"source":["### Exercise 2: TF-IDF\n","\n","**Task:** Use the `TfidfVectorizer` from scikit-learn to transform the following text corpus into a TF-IDF representation:\n","\n","```python\n","documents = [\n","    \"Natural language processing is fun.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","```\n"]},{"cell_type":"markdown","source":["TF-IDF is a numerical statistic that reflects how important a word is to a document in a corpus. It's a product of two terms: Term Frequency (TF) and Inverse Document Frequency (IDF).\n","\n","- **Term Frequency (TF):** This measures how frequently a term appears in a document. The more often a word appears, the higher its TF score, implying it's important to that document.\n","- **Inverse Document Frequency (IDF):** This measures how rare a term is across all documents in the corpus. Words that appear frequently in many documents (like \"the\", \"is\") will have a low IDF score, making them less important. Words that are rare across the corpus will have a high IDF score, indicating they are more distinctive to certain documents. The TF-IDF score increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This helps to filter out common words that don't carry much meaning."],"metadata":{"id":"iaKU00Pgslmv"}},{"cell_type":"code","metadata":{"id":"d954d4c6"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Natural language processing is fun.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","\n","# Initialize the TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","tfidf_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","print(\"Vocabulary:\")\n","print(vocab)\n","print(\"\\nTF-IDF Array:\")\n","print(tfidf_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a874e8be"},"source":["### Challenge 2: TF-IDF\n","\n","**Task:** Using the `vectorizer` from Exercise 2, transform the following new document into its TF-IDF representation and print the result:\n","\n","```python\n","new_document_tfidf = [\"NLP models are important.\"]\n","```\n","\n","**Hint:** Similar to the BoW challenge, use the `transform` method on the `vectorizer` that was already fitted on the original corpus."]}]}