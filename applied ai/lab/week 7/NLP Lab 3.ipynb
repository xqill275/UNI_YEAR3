{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI6t3KhGYWVB"
   },
   "outputs": [],
   "source": [
    "# This notebook contains a series of exercises designed to help you\n",
    "# understand and implement fundamental techniques in Natural Language Processing (NLP) feature engineering using scikit-learn.\n",
    "\n",
    "# Exercises:\n",
    "# 1.  **Bag of Words:** Learn how to transform text data into a Bag of Words representation.\n",
    "# 2.  **TF-IDF:** Explore the Term Frequency-Inverse Document Frequency (TF-IDF) method for text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a02be73"
   },
   "source": [
    "### Exercise 1: Bag of Words\n",
    "\n",
    "**Task:** Use the `CountVectorizer` from scikit-learn to transform the following text corpus into a Bag of Words representation:\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    \"Text processing is important for NLP.\",\n",
    "    \"Bag of Words is a simple text representation method.\",\n",
    "    \"Feature engineering is essential in machine learning.\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vwWWEqFsQ5L"
   },
   "source": [
    "The Bag of Words model represents text as an unordered collection of words, disregarding grammar and even word order, but keeping track of word frequencies. Essentially, it creates a vocabulary of all unique words in the corpus and then for each document, it counts how many times each word from the vocabulary appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e15c702"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"Text processing is important for NLP.\",\n",
    "    \"Bag of Words is a simple text representation method.\",\n",
    "    \"Feature engineering is essential in machine learning.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nBag of Words Array:\")\n",
    "print(bow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9387d77"
   },
   "source": [
    "### Challenge 1: Bag of Words\n",
    "\n",
    "**Task:** Using the `vectorizer` from Exercise 1, transform the following new document into its Bag of Words representation and print the result:\n",
    "\n",
    "```python\n",
    "new_document_bow = [\"NLP is an important field.\"]\n",
    "```\n",
    "\n",
    "**Hint:** You'll need to use the `transform` method on the `vectorizer` that was already fitted on the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.62276601,\n",
       "        0.        , 0.4736296 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.62276601, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document_bow = [\"NLP is an important field.\"]\n",
    "vectorizer.transform(new_document_bow).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8916b9b3"
   },
   "source": [
    "### Exercise 2: TF-IDF\n",
    "\n",
    "**Task:** Use the `TfidfVectorizer` from scikit-learn to transform the following text corpus into a TF-IDF representation:\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    \"Natural language processing is fun.\",\n",
    "    \"Language models are important in NLP.\",\n",
    "    \"Machine learning and NLP are closely related.\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaKU00Pgslmv"
   },
   "source": [
    "TF-IDF is a numerical statistic that reflects how important a word is to a document in a corpus. It's a product of two terms: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "- **Term Frequency (TF):** This measures how frequently a term appears in a document. The more often a word appears, the higher its TF score, implying it's important to that document.\n",
    "- **Inverse Document Frequency (IDF):** This measures how rare a term is across all documents in the corpus. Words that appear frequently in many documents (like \"the\", \"is\") will have a low IDF score, making them less important. Words that are rare across the corpus will have a high IDF score, indicating they are more distinctive to certain documents. The TF-IDF score increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This helps to filter out common words that don't carry much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d954d4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['and' 'are' 'closely' 'fun' 'important' 'in' 'is' 'language' 'learning'\n",
      " 'machine' 'models' 'natural' 'nlp' 'processing' 'related']\n",
      "\n",
      "TF-IDF Array:\n",
      "[[0.         0.         0.         0.46735098 0.         0.\n",
      "  0.46735098 0.35543247 0.         0.         0.         0.46735098\n",
      "  0.         0.46735098 0.        ]\n",
      " [0.         0.34949812 0.         0.         0.45954803 0.45954803\n",
      "  0.         0.34949812 0.         0.         0.45954803 0.\n",
      "  0.34949812 0.         0.        ]\n",
      " [0.40301621 0.30650422 0.40301621 0.         0.         0.\n",
      "  0.         0.         0.40301621 0.40301621 0.         0.\n",
      "  0.30650422 0.         0.40301621]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"Natural language processing is fun.\",\n",
    "    \"Language models are important in NLP.\",\n",
    "    \"Machine learning and NLP are closely related.\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array\n",
    "tfidf_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nTF-IDF Array:\")\n",
    "print(tfidf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a874e8be"
   },
   "source": [
    "### Challenge 2: TF-IDF\n",
    "\n",
    "**Task:** Using the `vectorizer` from Exercise 2, transform the following new document into its TF-IDF representation and print the result:\n",
    "\n",
    "```python\n",
    "new_document_tfidf = [\"NLP models are important.\"]\n",
    "```\n",
    "\n",
    "**Hint:** Similar to the BoW challenge, use the `transform` method on the `vectorizer` that was already fitted on the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42804604, 0.        , 0.        , 0.5628291 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5628291 , 0.        , 0.42804604, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document_tfidf = [\"NLP models are important.\"]\n",
    "\n",
    "vectorizer.transform(new_document_tfidf).toarray()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOfkH2jpRENxqrWAEztPjp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
