{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGXV9tiRZVjXdlvxp/4eeJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dXrB6FE9ZViG"},"outputs":[],"source":["# This notebook contains two exercises demonstrating fundamental concepts in Natural Language Processing:\n","\n","# N-grams: An exercise to generate trigrams (3-grams) from a given text using the NLTK library.\n","# Bigram Language Model: An exercise to train a simple bigram language model on a text corpus and calculate the probability of a specific bigram."]},{"cell_type":"markdown","metadata":{"id":"2b8db513"},"source":["## N-grams and Bigram Language Model\n","\n","### What are N-grams?\n","\n","N-grams are contiguous sequences of N items (words, characters, or phonemes) from a given sample of text or speech. They are widely used in NLP for various tasks such as language modelling, spelling correction, text prediction, and machine translation.\n","\n","*   **Unigram (N=1)**: A single word. E.g., \"Natural\", \"Language\", \"Processing\"\n","*   **Bigram (N=2)**: A sequence of two words. E.g., \"Natural Language\", \"Language Processing\"\n","*   **Trigram (N=3)**: A sequence of three words. E.g., \"Natural Language Processing\"\n","\n","**How they work:** N-grams capture the local context of words. By analyzing the frequency of these sequences, we can understand common word patterns and relationships in a text.\n","\n","### What is a Bigram Language Model?\n","\n","A Bigram Language Model is a type of probabilistic language model that predicts the probability of a word given the immediately preceding word. It's based on the idea that the probability of a word appearing depends only on the previous word, rather than the entire history of words in the sentence (this is known as the Markov assumption).\n","\n","**Formula:**\n","\n","The probability of a word $W_i$ given the previous word $W_{i-1}$ is calculated as:\n","\n","$$P(W_i | W_{i-1}) = \\frac{\\text{Count}(W_{i-1}, W_i)}{\\text{Count}(W_{i-1})}$$\n","\n","Where:\n","*   $\\text{Count}(W_{i-1}, W_i)$ is the number of times the bigram $(W_{i-1}, W_i)$ appears in the corpus.\n","*   $\\text{Count}(W_{i-1})$ is the number of times the unigram $(W_{i-1})$ appears in the corpus.\n","\n","**How it works:** To train a bigram language model, you count the occurrences of all bigrams and all unigrams in a given text corpus. Then, for any given bigram, you can calculate its probability using the formula above. This model can be extended to trigrams, quadrigrams, and so on, to capture more context, though data sparsity becomes a greater challenge with larger N values."]},{"cell_type":"markdown","metadata":{"id":"b9bb41e4"},"source":["## Exercise 1: N-grams\n","\n","**Task**: Generate trigrams (3-grams) from the following text: \"Natural Language Processing with Python.\""]},{"cell_type":"code","metadata":{"id":"64a5c1f2"},"source":["from nltk import ngrams\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# Sample text\n","text = \"Natural Language Processing with Python\"\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","\n","# Generate trigrams\n","trigrams = ngrams(tokens, 3)\n","\n","print(\"Trigrams:\")\n","for grams in trigrams:\n","    print(grams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"293bf364"},"source":["### Challenge 1: Trigram Frequency\n","\n","**Task**: Modify Exercise 1 to count the frequency of each unique trigram generated from the text and print the trigrams along with their counts.\n","\n","**Expected Output:**\n","```\n","Trigram Frequencies:\n","('Natural', 'Language', 'Processing'): 1\n","('Language', 'Processing', 'with'): 1\n","('Processing', 'with', 'Python'): 1\n","```\n","\n","**Hint**: Use Python's `collections.Counter` to efficiently count the occurrences of unique n-grams within a given text."]},{"cell_type":"markdown","metadata":{"id":"dcf4d6c1"},"source":["## Exercise 2: Bigram Language Model\n","\n","**Task**: Train a bigram language model on the following text corpus and calculate the probability of the bigram (\"Language\", \"Processing\"):\n","\n","```python\n","corpus = [\n","    \"Natural Language Processing is fascinating.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","```"]},{"cell_type":"code","metadata":{"id":"ed76062f"},"source":["from collections import defaultdict\n","import numpy as np\n","from nltk import ngrams\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# Sample text corpus\n","corpus = [\n","    \"Natural Language Processing is fascinating.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","\n","# Tokenize the text into words\n","tokenized_corpus = [nltk.word_tokenize(sentence) for sentence in corpus]\n","\n","# Function to calculate bigram probabilities\n","def train_bigram_model(tokenized_corpus):\n","    model = defaultdict(lambda: defaultdict(lambda: 0))\n","    # Count bigrams\n","    for sentence in tokenized_corpus:\n","        for w1, w2 in ngrams(sentence, 2):\n","            model[w1][w2] += 1\n","\n","    # Calculate probabilities\n","    for w1 in model:\n","        total_count = float(sum(model[w1].values()))\n","        for w2 in model[w1]:\n","            model[w1][w2] /= total_count\n","    return model\n","\n","# Train the bigram model\n","bigram_model = train_bigram_model(tokenized_corpus)\n","\n","# Function to get the probability of a bigram\n","def get_bigram_probability(bigram_model, w1, w2):\n","    return bigram_model[w1][w2]\n","\n","print(\"Bigram Probability (Processing | Language):\")\n","print(get_bigram_probability(bigram_model, 'Language', 'Processing'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2557f628"},"source":["### Challenge 2: Bigram Probability Calculation\n","\n","**Task**: Calculate the probability of the bigram ('models', 'are') using the trained bigram language model.\n","\n","**Expected Output:**\n","```\n","Bigram Probability ('models', 'are'):\n","1.0\n","```"]}]}