{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6ZJiSsLYFqEggyHFG6ZF1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"44845405"},"source":["## Exercise 1: Part-of-Speech Tagging with NLTK\n","\n","**Task**: Identify the grammatical category (Part-of-Speech) for each word in a given sentence.\n","**Library**: NLTK (Natural Language Toolkit) is widely used for linguistic annotation tasks."]},{"cell_type":"code","metadata":{"id":"f21cd744"},"source":["import nltk\n","nltk.download('averaged_perceptron_tagger_eng') # Required for POS tagging for English\n","from nltk.tokenize import word_tokenize\n","\n","print(\"\\n--- Exercise 1: Part-of-Speech Tagging with NLTK ---\")\n","text5 = \"The quick brown fox jumps over the lazy dog.\"\n","tokens5 = word_tokenize(text5)\n","pos_tags = nltk.pos_tag(tokens5) # Perform POS tagging on the tokens\n","print(f\"Original text: '{text5}'\")\n","print(f\"Tokens: {tokens5}\")\n","print(f\"POS Tags: {pos_tags}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The abbreviations like DT, JJ, NN, etc., are Part-of-Speech (POS) tags used by NLTK. They represent the grammatical category of each word. Here's a breakdown of the ones you've seen and some common ones:\n","\n","- DT: Determiner (e.g., \"The\", \"a\", \"an\")\n","- JJ: Adjective (e.g., \"quick\", \"brown\", \"lazy\", \"new\", \"brilliant\", \"few\", \"slow\")\n","- NN: Noun, singular or mass (e.g., \"fox\", \"dog\", \"movie\", \"acting\", \"storyline\", \"edge\", \"seat\", \"bit\")\n","- VBZ: Verb, 3rd person singular present (e.g., \"jumps\", \"is\")\n","- IN: Preposition or subordinating conjunction (e.g., \"over\", \"on\", \"of\", \"though\")\n","- .: Punctuation mark, sentence closer (e.g., \".\", \"!\")\n","- RB: Adverb (e.g., \"absolutely\", \"highly\")\n","- VBD: Verb, past tense (e.g., \"was\", \"kept\", \"were\")\n","- VBN: Verb, past participle (e.g., \"superb\" - often tagged as VBN when acting as an adjective derived from a verb)\n","- PRP: Personal pronoun (e.g., \"me\", \"I\", \"it\")\n","- PRP$: Possessive pronoun (e.g., \"my\")\n","- VBP: Verb, non-3rd person singular present (e.g., \"recommend\")\n","- CC: Coordinating conjunction (e.g., \"and\")\n","- NNS: Noun, plural (e.g., \"scenes\")\n","\n","These tags help in understanding the grammatical structure of sentences, which is foundational for many advanced NLP tasks!"],"metadata":{"id":"jmfRjFFd2Sl2"}},{"cell_type":"markdown","metadata":{"id":"51a80379"},"source":["## Exercise 2: Dependency Parsing with SpaCy\n","\n","**Task**: Analyze the grammatical structure of a sentence by identifying the head word for each word and the type of dependency relation between them.\n","**Library**: SpaCy provides efficient and accurate dependency parsing capabilities."]},{"cell_type":"code","metadata":{"id":"28c65f1a"},"source":["import spacy\n","\n","print(\"\\n--- Exercise 2: Dependency Parsing with SpaCy ---\")\n","# Load SpaCy model - ensure 'en_core_web_sm' is downloaded\n","try:\n","    nlp = spacy.load(\"en_core_web_sm\")\n","except OSError:\n","    print(\"Downloading en_core_web_sm model for SpaCy...\")\n","    from spacy.cli import download\n","    download(\"en_core_web_sm\")\n","    nlp = spacy.load(\"en_core_web_sm\")\n","\n","text7 = \"Apple is looking at buying U.K. startup for $1 billion.\"\n","doc = nlp(text7) # Process the text with the loaded SpaCy model\n","\n","print(f\"Original text: '{text7}'\")\n","print(\"Dependency Parse:\")\n","for token in doc:\n","    # Print the token, its part-of-speech, its head word, and the dependency relation\n","    print(f\"  {token.text:<10} {token.pos_:<10} {token.dep_:<15} {token.head.text:<10}\")\n","\n","# You can also visualize the dependency tree\n","# For visualization, you might need to install 'displacy'\n","# import displacy\n","# displacy.render(doc, style=\"dep\", jupyter=True, options={'compact': True, 'distance': 90})\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ad2b44a"},"source":["## Exercise 3: Text Classification with scikit-learn\n","\n","**Task**: Categorize text into predefined classes (e.g., positive/negative sentiment).\n","**Library**: `scikit-learn` is a powerful and widely used machine learning library in Python."]},{"cell_type":"code","metadata":{"id":"d4888a95"},"source":["import nltk\n","nltk.download('stopwords') # Often useful for text preprocessing in classification\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","print(\"\\n--- Exercise 3: Text Classification with scikit-learn ---\")\n","\n","# Sample Dataset (simple movie review sentiments)\n","texts = [\n","    \"This movie is fantastic and I love it!\",\n","    \"What a terrible film, absolutely dreadful.\",\n","    \"The acting was good, but the plot was boring.\",\n","    \"A truly amazing experience, highly recommended.\",\n","    \"I hated every minute of this movie, so bad.\",\n","    \"It was an okay film, nothing special.\"\n","]\n","labels = ['positive', 'negative', 'neutral', 'positive', 'negative', 'neutral'] # Corresponding labels\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n","\n","# Create a pipeline: Vectorizer -> Classifier\n","# CountVectorizer converts text into a matrix of token counts\n","# MultinomialNB (Naive Bayes) is a simple, yet effective classifier for text\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', MultinomialNB()),\n","])\n","\n","# Train the classifier\n","text_clf.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","predicted = text_clf.predict(X_test)\n","\n","print(f\"Original text for prediction: '{X_test}'\")\n","print(f\"Actual labels: {y_test}\")\n","print(f\"Predicted labels: {predicted}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, predicted, zero_division=0)) # Print classification metrics\n","\n","# Predict on a new unseen sentence\n","new_sentence = \"I really enjoyed this production, very entertaining!\"\n","predicted_sentiment = text_clf.predict([new_sentence])\n","print(f\"\\nNew sentence: '{new_sentence}'\")\n","print(f\"Predicted sentiment: {predicted_sentiment[0]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1fafd9a"},"source":["## Challenge: Integrated NLP Pipeline\n","\n","**Task**: Take a new paragraph of text, perform several NLP steps on it, and then classify its sentiment. This exercise integrates concepts from Part-of-Speech Tagging, Dependency Parsing, and Text Classification.\n","\n","**Steps:**\n","1.  **Choose a new paragraph of text.**\n","2.  **Perform Tokenization and Part-of-Speech Tagging** using NLTK (similar to Exercise 1).\n","3.  **Perform Dependency Parsing** using SpaCy to analyze the grammatical structure (similar to Exercise 2).\n","4.  **Classify the sentiment** of the paragraph using the pre-trained `text_clf` model from Exercise 3. You might need to consider how to apply a sentence-level classifier to a paragraph."]}]}