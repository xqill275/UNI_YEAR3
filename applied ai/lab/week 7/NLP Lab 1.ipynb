{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FH3GMNUyHHP7"
   },
   "outputs": [],
   "source": [
    "# This notebook demonstrates fundamental Natural Language Processing (NLP) tasks using popular Python libraries.\n",
    "# Each exercise focuses on a specific NLP technique:\n",
    "# 1.  **Tokenization** with NLTK\n",
    "# 2.  **Named Entity Recognition (NER)** with SpaCy\n",
    "# 3.  **Sentiment Analysis** with TextBlob\n",
    "# 4.  **Text Summarization** with Sumy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51e8144d"
   },
   "source": [
    "### Explanation of NLP Concepts:\n",
    "\n",
    "1.  **Tokenization** with NLTK: The process of breaking down a text into smaller units called tokens, which can be words, subwords, or characters. NLTK provides tools like `word_tokenize` for this task.\n",
    "\n",
    "2.  **Named Entity Recognition (NER)** with SpaCy: A technique to identify and classify named entities in text into predefined categories such as person names, organizations, locations, expressions of times, quantities, monetary values, etc. SpaCy is a library widely used for this purpose.\n",
    "\n",
    "3.  **Sentiment Analysis** with TextBlob: The computational study of opinions, sentiments, and emotions expressed in text. It determines the emotional tone behind a piece of text, often classifying it as positive, negative, or neutral. TextBlob offers a simple API for sentiment analysis.\n",
    "\n",
    "4.  **Text Summarization** with Sumy: The process of condensing a longer text into a shorter, coherent, and fluent version while retaining the most important information and overall meaning of the original text. Sumy is a Python library that provides various summarization algorithms, like LSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ce59bdc"
   },
   "source": [
    "## Import Libraries and Download Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dbbd2a6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Required for NLTK's word tokenizer\n",
    "nltk.download('punkt_tab') # Required for some NLTK tokenizer functionalities (e.g., used by Sumy)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27805d67"
   },
   "source": [
    "## Install `sumy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d1bb5134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sumy) (2.32.5)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sumy) (3.9.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from breadability>=0.1.20->sumy) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.2->sumy) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.7.0->sumy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.7.0->sumy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.7.0->sumy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olive\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\olive\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install sumy if not already installed\n",
    "!pip install sumy\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "nltk.download('stopwords') # Sumy often uses NLTK stopwords for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6a4fd0b"
   },
   "source": [
    "## Exercise 1: Tokenization with NLTK\n",
    "\n",
    "**Task**: Break down a given text into individual words or tokens.\n",
    "**Library**: NLTK (Natural Language Toolkit) is a powerful library for working with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ffadef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exercise 1: Tokenization with NLTK ---\n",
      "Original text: 'Natural Language Processing enables computers to understand human language.'\n",
      "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Exercise 1: Tokenization with NLTK ---\")\n",
    "text1 = \"Natural Language Processing enables computers to understand human language.\"\n",
    "tokens = word_tokenize(text1) # Uses NLTK's word_tokenize function to split the text\n",
    "print(f\"Original text: '{text1}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d81172bb"
   },
   "source": [
    "### Challenge 1: Tokenization\n",
    "\n",
    "**Task**: Experiment with different NLTK tokenizers. For example, try `wordpunct_tokenize` or `TreebankWordTokenizer` (after importing if necessary) and observe the differences in tokenization for the given text or a new sentence of your choice. Pay attention to how punctuation is handled.\n",
    "\n",
    "**Hint**: You might need to import `nltk.tokenize.wordpunct_tokenize` or `nltk.tokenize.TreebankWordTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using wordpunct_tokenize:\n",
      "Tokens: ['Hello', ',', 'world', '!', 'Let', \"'\", 's', 'explore', 'tokenization', ':', 'NLTK', \"'\", 's', 'way', '.']\n",
      "\n",
      "Using TreebankWordTokenizer:\n",
      "Tokens: ['Hello', ',', 'world', '!', 'Let', \"'s\", 'explore', 'tokenization', ':', 'NLTK', \"'s\", 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TreebankWordTokenizer\n",
    "text_challenge = \"Hello, world! Let's explore tokenization: NLTK's way.\"\n",
    "# Using wordpunct_tokenize\n",
    "tokens_wordpunct = wordpunct_tokenize(text_challenge)\n",
    "print(f\"\\nUsing wordpunct_tokenize:\\nTokens: {tokens_wordpunct}\")\n",
    "# Using TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "tokens_treebank = treebank_tokenizer.tokenize(text_challenge)\n",
    "print(f\"\\nUsing TreebankWordTokenizer:\\nTokens: {tokens_treebank}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba596589"
   },
   "source": [
    "## Exercise 2: Named Entity Recognition with SpaCy\n",
    "\n",
    "**Task**: Identify and classify named entities (like persons, organizations, locations) in text.\n",
    "**Library**: SpaCy is an industrial-strength natural language processing library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0a2fa08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exercise 2: Named Entity Recognition with SpaCy ---\n",
      "Original text: 'Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.'\n",
      "Named Entities:\n",
      "  Google               ORG\n",
      "  Larry Page           PERSON\n",
      "  Sergey Brin          PERSON\n",
      "  Ph.D.                WORK_OF_ART\n",
      "  Stanford University  ORG\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Exercise 2: Named Entity Recognition with SpaCy ---\")\n",
    "# Load SpaCy model - ensure 'en_core_web_sm' is downloaded (you might need !python -m spacy download en_core_web_sm)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # Loads a small English model for processing\n",
    "except OSError:\n",
    "    print(\"Downloading en_core_web_sm model for SpaCy...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\") # If model is not found, download it automatically\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text2 = \"Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\"\n",
    "doc = nlp(text2) # Process the text with the loaded SpaCy model\n",
    "print(f\"Original text: '{text2}'\")\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents: # Iterate through the detected entities\n",
    "    print(f\"  {ent.text:<20} {ent.label_}\") # Print the entity text and its label (e.g., PERSON, ORG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4f7c749"
   },
   "source": [
    "### Challenge 2: Named Entity Recognition\n",
    "\n",
    "**Task**: Apply NER to a new sentence that contains different types of entities (e.g., dates, monetary values, products). Analyze the output and see if SpaCy correctly identifies and labels them. What happens if you use a sentence with less common entities?\n",
    "\n",
    "**Example Sentence**: \"Apple released the iPhone 15 in September 2023 for $799. The event took place in Cupertino, California.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Challenge NER text: 'Apple released the iPhone 15 in September 2023 for $799. The event took place in Cupertino, California.'\n",
      "Named Entities:\n",
      "  Apple                ORG\n",
      "  September 2023       DATE\n",
      "  799                  MONEY\n",
      "  Cupertino            GPE\n",
      "  California           GPE\n",
      "\n",
      "--- Exercise 3: Sentiment Analysis with TextBlob ---\n",
      "Original text: 'I love natural language processing! It's incredibly fascinating and fun.'\n",
      "Sentiment Polarity: 0.40625, Subjectivity: 0.5125000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_challenge_ner = \"Apple released the iPhone 15 in September 2023 for $799. The event took place in Cupertino, California.\"\n",
    "doc_challenge = nlp(text_challenge_ner)\n",
    "print(f\"\\nChallenge NER text: '{text_challenge_ner}'\")\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc_challenge.ents:\n",
    "    print(f\"  {ent.text:<20} {ent.label_}\")\n",
    "print(\"\\n--- Exercise 3: Sentiment Analysis with TextBlob ---\")\n",
    "text3 = \"I love natural language processing! It's incredibly fascinating and fun.\"\n",
    "blob = TextBlob(text3) # Create a TextBlob object for sentiment analysis\n",
    "print(f\"Original text: '{text3}'\")\n",
    "print(f\"Sentiment Polarity: {blob.sentiment.polarity}, Subjectivity: {blob.sentiment.subjectivity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de626309"
   },
   "source": [
    "## Exercise 3: Sentiment Analysis with TextBlob\n",
    "\n",
    "**Task**: Determine the emotional tone behind a piece of text, usually categorizing it as positive, negative, or neutral.\n",
    "**Library**: TextBlob is a simple Python library for processing textual data. It provides a simple API for common NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5e5bd906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exercise 3: Sentiment Analysis with TextBlob ---\n",
      "Original text: 'I am extremely happy with the service provided.'\n",
      "Sentiment: Sentiment(polarity=0.8, subjectivity=1.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Exercise 3: Sentiment Analysis with TextBlob ---\")\n",
    "text3 = \"I am extremely happy with the service provided.\"\n",
    "blob = TextBlob(text3) # Create a TextBlob object from the text\n",
    "sentiment = blob.sentiment # Access the sentiment property, which returns polarity and subjectivity\n",
    "print(f\"Original text: '{text3}'\")\n",
    "print(f\"Sentiment: {sentiment}\") # Polarity ranges from -1 (negative) to 1 (positive), Subjectivity from 0 (objective) to 1 (subjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6117e393"
   },
   "source": [
    "### Challenge 3: Sentiment Analysis\n",
    "\n",
    "**Task**: Choose a short paragraph from a product review or a news article. Perform sentiment analysis using TextBlob. Analyze the `polarity` and `subjectivity` scores. How well does it align with your own understanding of the text's sentiment? Try sentences with sarcasm or nuanced language and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Challenge Sentiment text: 'Articles combine with nouns to form noun phrases, and typically specify the grammatical definiteness of the noun phrase. In English, the and a (rendered as an when followed by a vowel sound) are the definite and indefinite articles respectively. Articles in many other languages also carry additional grammatical information such as gender, number, and case. Articles are part of a broader category called determiners, which also include demonstratives, possessive determiners, and quantifiers. In linguistic interlinear glossing, articles are abbreviated as ART. '\n",
      "Sentiment: Sentiment(polarity=0.0787037037037037, subjectivity=0.33055555555555555)\n"
     ]
    }
   ],
   "source": [
    "''' ### Challenge 3: Sentiment Analysis\n",
    "\n",
    "**Task**: Choose a short paragraph from a product review or a news article. Perform sentiment analysis using TextBlob. Analyze the `polarity` and `subjectivity` scores. How well does it align with your own understanding of the text's sentiment? Try sentences with sarcasm or nuanced language and observe the results.'''\n",
    "\n",
    "text_challenge_sentiment = \"Articles combine with nouns to form noun phrases, and typically specify the grammatical definiteness of the noun phrase. In English, the and a (rendered as an when followed by a vowel sound) are the definite and indefinite articles respectively. Articles in many other languages also carry additional grammatical information such as gender, number, and case. Articles are part of a broader category called determiners, which also include demonstratives, possessive determiners, and quantifiers. In linguistic interlinear glossing, articles are abbreviated as ART. \"\n",
    "blob_challenge = TextBlob(text_challenge_sentiment)\n",
    "sentiment_challenge = blob_challenge.sentiment\n",
    "print(f\"\\nChallenge Sentiment text: '{text_challenge_sentiment}'\")\n",
    "print(f\"Sentiment: {sentiment_challenge}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f43ac1e"
   },
   "source": [
    "## Exercise 4: Text Summarization with Sumy\n",
    "\n",
    "**Task**: Condense a longer text into a shorter version while retaining the most important information.\n",
    "**Library**: Sumy is a Python library for automatic text summarization of text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ed137d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exercise 4: Text Summarization with Sumy ---\n",
      "Original text (first 100 chars): 'Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, ar...' \n",
      "Summary (2 sentences):\n",
      "  - Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, artificial intelligence, and linguistics.\n",
      "  - Its applications are constantly expanding, making it a critical area of research and development in today's technologically driven world.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Exercise 4: Text Summarization with Sumy ---\")\n",
    "text4 = \"Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, artificial intelligence, and linguistics. It enables machines to understand, interpret, and generate human language, opening up a world of possibilities for applications ranging from chatbots and translation services to sentiment analysis and beyond. This field involves various techniques, including machine learning, deep learning, and rule-based methods, to process and analyze large amounts of text data. The goal of NLP is to bridge the communication gap between humans and computers, allowing for more natural and intuitive interactions. Its applications are constantly expanding, making it a critical area of research and development in today's technologically driven world.\"\n",
    "parser = PlaintextParser.from_string(text4, Tokenizer(\"english\")) # Parse the text using Sumy's PlaintextParser and English tokenizer\n",
    "summarizer = LsaSummarizer() # Initialize an LSA (Latent Semantic Analysis) summarizer\n",
    "summary = summarizer(parser.document, 2)  # Summarize the document into 2 sentences\n",
    "print(f\"Original text (first 100 chars): '{text4[:100]}...' \")\n",
    "print(\"Summary (2 sentences):\")\n",
    "for sentence in summary:\n",
    "    print(f\"  - {sentence}\") # Print each sentence of the generated summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bf1b305"
   },
   "source": [
    "### Challenge 4: Text Summarization\n",
    "\n",
    "**Task**: Take a longer article or a portion of text (e.g., from Wikipedia) and apply Sumy's LSA summarizer. Experiment with different numbers of sentences for the summary (e.g., 3, 5). Compare the generated summaries to see which one best captures the essence of the original text while remaining concise. You can also explore other summarization algorithms available in Sumy if you're feeling adventurous (e.g., `LexRankSummarizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Challenge NER text: 'A film[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations.[1]Films are produced by recording actual people and objects with cameras or by creating them using animation techniques and special effects. They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer. Flickering between frames is not seen due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. The visual elements of cinema need no translation, giving the motion picture a universal power of communication. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the dialogue. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them. '\n",
      "Summary (3 sentences):\n",
      "  - A film[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations.\n",
      "  - They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer.\n",
      "  - Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_challenge_summary = \"A film[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations.[1]Films are produced by recording actual people and objects with cameras or by creating them using animation techniques and special effects. They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer. Flickering between frames is not seen due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. The visual elements of cinema need no translation, giving the motion picture a universal power of communication. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the dialogue. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them. \"\n",
    "parser_challenge = PlaintextParser.from_string(text_challenge_summary, Tokenizer(\"english\"))\n",
    "summarizer_challenge = LsaSummarizer()\n",
    "summary_challenge_3 = summarizer_challenge(parser_challenge.document, 3)  # Summarize into 3 sentences\n",
    "summary_challenge_5 = summarizer_challenge(parser_challenge.document, 5)\n",
    "print(f\"\\nChallenge NER text: '{text_challenge_summary}'\")\n",
    "print(\"Summary (3 sentences):\")\n",
    "for sentence in summary_challenge_3:\n",
    "    print(f\"  - {sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMmRCOLxfDThsaEkvHq5NVq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
